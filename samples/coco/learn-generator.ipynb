{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d95e9f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\anmol.deep\\\\PycharmProjects\\\\Mask_RCNN\\\\samples\\\\coco'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d309e3bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ANMOL~1.DEE\\AppData\\Local\\Temp/ipykernel_11480/4113846746.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mresize_image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "from ..mrcnn.utils import resize_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba56758",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f70540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_gt(dataset, config, image_id, augmentation=None):\n",
    "    \"\"\"Load and return ground truth data for an image (image, mask, bounding boxes).\n",
    "\n",
    "    augmentation: Optional. An imgaug (https://github.com/aleju/imgaug) augmentation.\n",
    "        For example, passing imgaug.augmenters.Fliplr(0.5) flips images\n",
    "        right/left 50% of the time.\n",
    "\n",
    "    Returns:\n",
    "    image: [height, width, 3]\n",
    "    shape: the original shape of the image before resizing and cropping.\n",
    "    class_ids: [instance_count] Integer class IDs\n",
    "    bbox: [instance_count, (y1, x1, y2, x2)]\n",
    "    mask: [height, width, instance_count]. The height and width are those\n",
    "        of the image unless use_mini_mask is True, in which case they are\n",
    "        defined in MINI_MASK_SHAPE.\n",
    "    \"\"\"\n",
    "    # Load image and mask\n",
    "    image = dataset.load_image(image_id)\n",
    "    mask, class_ids = dataset.load_mask(image_id)\n",
    "    original_shape = image.shape\n",
    "    image, window, scale, padding, crop = utils.resize_image(\n",
    "        image,\n",
    "        min_dim=config.IMAGE_MIN_DIM,\n",
    "        min_scale=config.IMAGE_MIN_SCALE,\n",
    "        max_dim=config.IMAGE_MAX_DIM,\n",
    "        mode=config.IMAGE_RESIZE_MODE)\n",
    "    mask = utils.resize_mask(mask, scale, padding, crop)\n",
    "\n",
    "    # Augmentation\n",
    "    # This requires the imgaug lib (https://github.com/aleju/imgaug)\n",
    "    if augmentation:\n",
    "        import imgaug\n",
    "\n",
    "        # Augmenters that are safe to apply to masks\n",
    "        # Some, such as Affine, have settings that make them unsafe, so always\n",
    "        # test your augmentation on masks\n",
    "        MASK_AUGMENTERS = [\"Sequential\", \"SomeOf\", \"OneOf\", \"Sometimes\",\n",
    "                           \"Fliplr\", \"Flipud\", \"CropAndPad\",\n",
    "                           \"Affine\", \"PiecewiseAffine\"]\n",
    "\n",
    "        def hook(images, augmenter, parents, default):\n",
    "            \"\"\"Determines which augmenters to apply to masks.\"\"\"\n",
    "            return augmenter.__class__.__name__ in MASK_AUGMENTERS\n",
    "\n",
    "        # Store shapes before augmentation to compare\n",
    "        image_shape = image.shape\n",
    "        mask_shape = mask.shape\n",
    "        # Make augmenters deterministic to apply similarly to images and masks\n",
    "        det = augmentation.to_deterministic()\n",
    "        image = det.augment_image(image)\n",
    "        # Change mask to np.uint8 because imgaug doesn't support np.bool\n",
    "        mask = det.augment_image(mask.astype(np.uint8),\n",
    "                                 hooks=imgaug.HooksImages(activator=hook))\n",
    "        # Verify that shapes didn't change\n",
    "        assert image.shape == image_shape, \"Augmentation shouldn't change image size\"\n",
    "        assert mask.shape == mask_shape, \"Augmentation shouldn't change mask size\"\n",
    "        # Change mask back to bool\n",
    "        mask = mask.astype(np.bool)\n",
    "\n",
    "    # Note that some boxes might be all zeros if the corresponding mask got cropped out.\n",
    "    # and here is to filter them out\n",
    "    _idx = np.sum(mask, axis=(0, 1)) > 0\n",
    "    mask = mask[:, :, _idx]\n",
    "    class_ids = class_ids[_idx]\n",
    "    # Bounding boxes. Note that some boxes might be all zeros\n",
    "    # if the corresponding mask got cropped out.\n",
    "    # bbox: [num_instances, (y1, x1, y2, x2)]\n",
    "    bbox = utils.extract_bboxes(mask)\n",
    "\n",
    "    # Active classes\n",
    "    # Different datasets have different classes, so track the\n",
    "    # classes supported in the dataset of this image.\n",
    "    active_class_ids = np.zeros([dataset.num_classes], dtype=np.int32)\n",
    "    source_class_ids = dataset.source_class_ids[dataset.image_info[image_id][\"source\"]]\n",
    "    active_class_ids[source_class_ids] = 1\n",
    "\n",
    "    # Resize masks to smaller size to reduce memory usage\n",
    "    if config.USE_MINI_MASK:\n",
    "        mask = utils.minimize_mask(bbox, mask, config.MINI_MASK_SHAPE)\n",
    "\n",
    "    # Image meta data\n",
    "    image_meta = compose_image_meta(image_id, original_shape, image.shape,\n",
    "                                    window, scale, active_class_ids)\n",
    "\n",
    "    return image, image_meta, class_ids, bbox, mask\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44640b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_detection_targets(rpn_rois, gt_class_ids, gt_boxes, gt_masks, config):\n",
    "    \"\"\"Generate targets for training Stage 2 classifier and mask heads.\n",
    "    This is not used in normal training. It's useful for debugging or to train\n",
    "    the Mask RCNN heads without using the RPN head.\n",
    "\n",
    "    Inputs:\n",
    "    rpn_rois: [N, (y1, x1, y2, x2)] proposal boxes.\n",
    "    gt_class_ids: [instance count] Integer class IDs\n",
    "    gt_boxes: [instance count, (y1, x1, y2, x2)]\n",
    "    gt_masks: [height, width, instance count] Ground truth masks. Can be full\n",
    "              size or mini-masks.\n",
    "\n",
    "    Returns:\n",
    "    rois: [TRAIN_ROIS_PER_IMAGE, (y1, x1, y2, x2)]\n",
    "    class_ids: [TRAIN_ROIS_PER_IMAGE]. Integer class IDs.\n",
    "    bboxes: [TRAIN_ROIS_PER_IMAGE, NUM_CLASSES, (y, x, log(h), log(w))]. Class-specific\n",
    "            bbox refinements.\n",
    "    masks: [TRAIN_ROIS_PER_IMAGE, height, width, NUM_CLASSES). Class specific masks cropped\n",
    "           to bbox boundaries and resized to neural network output size.\n",
    "    \"\"\"\n",
    "    assert rpn_rois.shape[0] > 0\n",
    "    assert gt_class_ids.dtype == np.int32, \"Expected int but got {}\".format(\n",
    "        gt_class_ids.dtype)\n",
    "    assert gt_boxes.dtype == np.int32, \"Expected int but got {}\".format(\n",
    "        gt_boxes.dtype)\n",
    "    assert gt_masks.dtype == np.bool_, \"Expected bool but got {}\".format(\n",
    "        gt_masks.dtype)\n",
    "\n",
    "    # It's common to add GT Boxes to ROIs but we don't do that here because\n",
    "    # according to XinLei Chen's paper, it doesn't help.\n",
    "\n",
    "    # Trim empty padding in gt_boxes and gt_masks parts\n",
    "    instance_ids = np.where(gt_class_ids > 0)[0]\n",
    "    assert instance_ids.shape[0] > 0, \"Image must contain instances.\"\n",
    "    gt_class_ids = gt_class_ids[instance_ids]\n",
    "    gt_boxes = gt_boxes[instance_ids]\n",
    "    gt_masks = gt_masks[:, :, instance_ids]\n",
    "\n",
    "    # Compute areas of ROIs and ground truth boxes.\n",
    "    rpn_roi_area = (rpn_rois[:, 2] - rpn_rois[:, 0]) * \\\n",
    "        (rpn_rois[:, 3] - rpn_rois[:, 1])\n",
    "    gt_box_area = (gt_boxes[:, 2] - gt_boxes[:, 0]) * \\\n",
    "        (gt_boxes[:, 3] - gt_boxes[:, 1])\n",
    "\n",
    "    # Compute overlaps [rpn_rois, gt_boxes]\n",
    "    overlaps = np.zeros((rpn_rois.shape[0], gt_boxes.shape[0]))\n",
    "    for i in range(overlaps.shape[1]):\n",
    "        gt = gt_boxes[i]\n",
    "        overlaps[:, i] = utils.compute_iou(\n",
    "            gt, rpn_rois, gt_box_area[i], rpn_roi_area)\n",
    "\n",
    "    # Assign ROIs to GT boxes\n",
    "    rpn_roi_iou_argmax = np.argmax(overlaps, axis=1)\n",
    "    rpn_roi_iou_max = overlaps[np.arange(\n",
    "        overlaps.shape[0]), rpn_roi_iou_argmax]\n",
    "    # GT box assigned to each ROI\n",
    "    rpn_roi_gt_boxes = gt_boxes[rpn_roi_iou_argmax]\n",
    "    rpn_roi_gt_class_ids = gt_class_ids[rpn_roi_iou_argmax]\n",
    "\n",
    "    # Positive ROIs are those with >= 0.5 IoU with a GT box.\n",
    "    fg_ids = np.where(rpn_roi_iou_max > 0.5)[0]\n",
    "\n",
    "    # Negative ROIs are those with max IoU 0.1-0.5 (hard example mining)\n",
    "    # TODO: To hard example mine or not to hard example mine, that's the question\n",
    "    # bg_ids = np.where((rpn_roi_iou_max >= 0.1) & (rpn_roi_iou_max < 0.5))[0]\n",
    "    bg_ids = np.where(rpn_roi_iou_max < 0.5)[0]\n",
    "\n",
    "    # Subsample ROIs. Aim for 33% foreground.\n",
    "    # FG\n",
    "    fg_roi_count = int(config.TRAIN_ROIS_PER_IMAGE * config.ROI_POSITIVE_RATIO)\n",
    "    if fg_ids.shape[0] > fg_roi_count:\n",
    "        keep_fg_ids = np.random.choice(fg_ids, fg_roi_count, replace=False)\n",
    "    else:\n",
    "        keep_fg_ids = fg_ids\n",
    "    # BG\n",
    "    remaining = config.TRAIN_ROIS_PER_IMAGE - keep_fg_ids.shape[0]\n",
    "    if bg_ids.shape[0] > remaining:\n",
    "        keep_bg_ids = np.random.choice(bg_ids, remaining, replace=False)\n",
    "    else:\n",
    "        keep_bg_ids = bg_ids\n",
    "    # Combine indices of ROIs to keep\n",
    "    keep = np.concatenate([keep_fg_ids, keep_bg_ids])\n",
    "    # Need more?\n",
    "    remaining = config.TRAIN_ROIS_PER_IMAGE - keep.shape[0]\n",
    "    if remaining > 0:\n",
    "        # Looks like we don't have enough samples to maintain the desired\n",
    "        # balance. Reduce requirements and fill in the rest. This is\n",
    "        # likely different from the Mask RCNN paper.\n",
    "\n",
    "        # There is a small chance we have neither fg nor bg samples.\n",
    "        if keep.shape[0] == 0:\n",
    "            # Pick bg regions with easier IoU threshold\n",
    "            bg_ids = np.where(rpn_roi_iou_max < 0.5)[0]\n",
    "            assert bg_ids.shape[0] >= remaining\n",
    "            keep_bg_ids = np.random.choice(bg_ids, remaining, replace=False)\n",
    "            assert keep_bg_ids.shape[0] == remaining\n",
    "            keep = np.concatenate([keep, keep_bg_ids])\n",
    "        else:\n",
    "            # Fill the rest with repeated bg rois.\n",
    "            keep_extra_ids = np.random.choice(\n",
    "                keep_bg_ids, remaining, replace=True)\n",
    "            keep = np.concatenate([keep, keep_extra_ids])\n",
    "    assert keep.shape[0] == config.TRAIN_ROIS_PER_IMAGE, \\\n",
    "        \"keep doesn't match ROI batch size {}, {}\".format(\n",
    "            keep.shape[0], config.TRAIN_ROIS_PER_IMAGE)\n",
    "\n",
    "    # Reset the gt boxes assigned to BG ROIs.\n",
    "    rpn_roi_gt_boxes[keep_bg_ids, :] = 0\n",
    "    rpn_roi_gt_class_ids[keep_bg_ids] = 0\n",
    "\n",
    "    # For each kept ROI, assign a class_id, and for FG ROIs also add bbox refinement.\n",
    "    rois = rpn_rois[keep]\n",
    "    roi_gt_boxes = rpn_roi_gt_boxes[keep]\n",
    "    roi_gt_class_ids = rpn_roi_gt_class_ids[keep]\n",
    "    roi_gt_assignment = rpn_roi_iou_argmax[keep]\n",
    "\n",
    "    # Class-aware bbox deltas. [y, x, log(h), log(w)]\n",
    "    bboxes = np.zeros((config.TRAIN_ROIS_PER_IMAGE,\n",
    "                       config.NUM_CLASSES, 4), dtype=np.float32)\n",
    "    pos_ids = np.where(roi_gt_class_ids > 0)[0]\n",
    "    bboxes[pos_ids, roi_gt_class_ids[pos_ids]] = utils.box_refinement(\n",
    "        rois[pos_ids], roi_gt_boxes[pos_ids, :4])\n",
    "    # Normalize bbox refinements\n",
    "    bboxes /= config.BBOX_STD_DEV\n",
    "\n",
    "    # Generate class-specific target masks\n",
    "    masks = np.zeros((config.TRAIN_ROIS_PER_IMAGE, config.MASK_SHAPE[0], config.MASK_SHAPE[1], config.NUM_CLASSES),\n",
    "                     dtype=np.float32)\n",
    "    for i in pos_ids:\n",
    "        class_id = roi_gt_class_ids[i]\n",
    "        assert class_id > 0, \"class id must be greater than 0\"\n",
    "        gt_id = roi_gt_assignment[i]\n",
    "        class_mask = gt_masks[:, :, gt_id]\n",
    "\n",
    "        if config.USE_MINI_MASK:\n",
    "            # Create a mask placeholder, the size of the image\n",
    "            placeholder = np.zeros(config.IMAGE_SHAPE[:2], dtype=bool)\n",
    "            # GT box\n",
    "            gt_y1, gt_x1, gt_y2, gt_x2 = gt_boxes[gt_id]\n",
    "            gt_w = gt_x2 - gt_x1\n",
    "            gt_h = gt_y2 - gt_y1\n",
    "            # Resize mini mask to size of GT box\n",
    "            placeholder[gt_y1:gt_y2, gt_x1:gt_x2] = \\\n",
    "                np.round(utils.resize(class_mask, (gt_h, gt_w))).astype(bool)\n",
    "            # Place the mini batch in the placeholder\n",
    "            class_mask = placeholder\n",
    "\n",
    "        # Pick part of the mask and resize it\n",
    "        y1, x1, y2, x2 = rois[i].astype(np.int32)\n",
    "        m = class_mask[y1:y2, x1:x2]\n",
    "        mask = utils.resize(m, config.MASK_SHAPE)\n",
    "        masks[i, :, :, class_id] = mask\n",
    "\n",
    "    return rois, roi_gt_class_ids, bboxes, masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48db5d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rpn_targets(image_shape, anchors, gt_class_ids, gt_boxes, config):\n",
    "    \"\"\"Given the anchors and GT boxes, compute overlaps and identify positive\n",
    "    anchors and deltas to refine them to match their corresponding GT boxes.\n",
    "\n",
    "    anchors: [num_anchors, (y1, x1, y2, x2)]\n",
    "    gt_class_ids: [num_gt_boxes] Integer class IDs.\n",
    "    gt_boxes: [num_gt_boxes, (y1, x1, y2, x2)]\n",
    "\n",
    "    Returns:\n",
    "    rpn_match: [N] (int32) matches between anchors and GT boxes.\n",
    "               1 = positive anchor, -1 = negative anchor, 0 = neutral\n",
    "    rpn_bbox: [N, (dy, dx, log(dh), log(dw))] Anchor bbox deltas.\n",
    "    \"\"\"\n",
    "    # RPN Match: 1 = positive anchor, -1 = negative anchor, 0 = neutral\n",
    "    rpn_match = np.zeros([anchors.shape[0]], dtype=np.int32)\n",
    "    # RPN bounding boxes: [max anchors per image, (dy, dx, log(dh), log(dw))]\n",
    "    rpn_bbox = np.zeros((config.RPN_TRAIN_ANCHORS_PER_IMAGE, 4))\n",
    "\n",
    "    # Handle COCO crowds\n",
    "    # A crowd box in COCO is a bounding box around several instances. Exclude\n",
    "    # them from training. A crowd box is given a negative class ID.\n",
    "    crowd_ix = np.where(gt_class_ids < 0)[0]\n",
    "    if crowd_ix.shape[0] > 0:\n",
    "        # Filter out crowds from ground truth class IDs and boxes\n",
    "        non_crowd_ix = np.where(gt_class_ids > 0)[0]\n",
    "        crowd_boxes = gt_boxes[crowd_ix]\n",
    "        gt_class_ids = gt_class_ids[non_crowd_ix]\n",
    "        gt_boxes = gt_boxes[non_crowd_ix]\n",
    "        # Compute overlaps with crowd boxes [anchors, crowds]\n",
    "        crowd_overlaps = utils.compute_overlaps(anchors, crowd_boxes)\n",
    "        crowd_iou_max = np.amax(crowd_overlaps, axis=1)\n",
    "        no_crowd_bool = (crowd_iou_max < 0.001)\n",
    "    else:\n",
    "        # All anchors don't intersect a crowd\n",
    "        no_crowd_bool = np.ones([anchors.shape[0]], dtype=bool)\n",
    "\n",
    "    # Compute overlaps [num_anchors, num_gt_boxes]\n",
    "    overlaps = utils.compute_overlaps(anchors, gt_boxes)\n",
    "\n",
    "    # Match anchors to GT Boxes\n",
    "    # If an anchor overlaps a GT box with IoU >= 0.7 then it's positive.\n",
    "    # If an anchor overlaps a GT box with IoU < 0.3 then it's negative.\n",
    "    # Neutral anchors are those that don't match the conditions above,\n",
    "    # and they don't influence the loss function.\n",
    "    # However, don't keep any GT box unmatched (rare, but happens). Instead,\n",
    "    # match it to the closest anchor (even if its max IoU is < 0.3).\n",
    "    #\n",
    "    # 1. Set negative anchors first. They get overwritten below if a GT box is\n",
    "    # matched to them. Skip boxes in crowd areas.\n",
    "    anchor_iou_argmax = np.argmax(overlaps, axis=1)\n",
    "    anchor_iou_max = overlaps[np.arange(overlaps.shape[0]), anchor_iou_argmax]\n",
    "    rpn_match[(anchor_iou_max < 0.3) & (no_crowd_bool)] = -1\n",
    "    # 2. Set an anchor for each GT box (regardless of IoU value).\n",
    "    # If multiple anchors have the same IoU match all of them\n",
    "    gt_iou_argmax = np.argwhere(overlaps == np.max(overlaps, axis=0))[:,0]\n",
    "    rpn_match[gt_iou_argmax] = 1\n",
    "    # 3. Set anchors with high overlap as positive.\n",
    "    rpn_match[anchor_iou_max >= 0.7] = 1\n",
    "\n",
    "    # Subsample to balance positive and negative anchors\n",
    "    # Don't let positives be more than half the anchors\n",
    "    ids = np.where(rpn_match == 1)[0]\n",
    "    extra = len(ids) - (config.RPN_TRAIN_ANCHORS_PER_IMAGE // 2)\n",
    "    if extra > 0:\n",
    "        # Reset the extra ones to neutral\n",
    "        ids = np.random.choice(ids, extra, replace=False)\n",
    "        rpn_match[ids] = 0\n",
    "    # Same for negative proposals\n",
    "    ids = np.where(rpn_match == -1)[0]\n",
    "    extra = len(ids) - (config.RPN_TRAIN_ANCHORS_PER_IMAGE -\n",
    "                        np.sum(rpn_match == 1))\n",
    "    if extra > 0:\n",
    "        # Rest the extra ones to neutral\n",
    "        ids = np.random.choice(ids, extra, replace=False)\n",
    "        rpn_match[ids] = 0\n",
    "\n",
    "    # For positive anchors, compute shift and scale needed to transform them\n",
    "    # to match the corresponding GT boxes.\n",
    "    ids = np.where(rpn_match == 1)[0]\n",
    "    ix = 0  # index into rpn_bbox\n",
    "    # TODO: use box_refinement() rather than duplicating the code here\n",
    "    for i, a in zip(ids, anchors[ids]):\n",
    "        # Closest gt box (it might have IoU < 0.7)\n",
    "        gt = gt_boxes[anchor_iou_argmax[i]]\n",
    "\n",
    "        # Convert coordinates to center plus width/height.\n",
    "        # GT Box\n",
    "        gt_h = gt[2] - gt[0]\n",
    "        gt_w = gt[3] - gt[1]\n",
    "        gt_center_y = gt[0] + 0.5 * gt_h\n",
    "        gt_center_x = gt[1] + 0.5 * gt_w\n",
    "        # Anchor\n",
    "        a_h = a[2] - a[0]\n",
    "        a_w = a[3] - a[1]\n",
    "        a_center_y = a[0] + 0.5 * a_h\n",
    "        a_center_x = a[1] + 0.5 * a_w\n",
    "\n",
    "        # Compute the bbox refinement that the RPN should predict.\n",
    "        rpn_bbox[ix] = [\n",
    "            (gt_center_y - a_center_y) / a_h,\n",
    "            (gt_center_x - a_center_x) / a_w,\n",
    "            np.log(gt_h / a_h),\n",
    "            np.log(gt_w / a_w),\n",
    "        ]\n",
    "        # Normalize\n",
    "        rpn_bbox[ix] /= config.RPN_BBOX_STD_DEV\n",
    "        ix += 1\n",
    "\n",
    "    return rpn_match, rpn_bbox\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3a65d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_rois(image_shape, count, gt_class_ids, gt_boxes):\n",
    "    \"\"\"Generates ROI proposals similar to what a region proposal network\n",
    "    would generate.\n",
    "\n",
    "    image_shape: [Height, Width, Depth]\n",
    "    count: Number of ROIs to generate\n",
    "    gt_class_ids: [N] Integer ground truth class IDs\n",
    "    gt_boxes: [N, (y1, x1, y2, x2)] Ground truth boxes in pixels.\n",
    "\n",
    "    Returns: [count, (y1, x1, y2, x2)] ROI boxes in pixels.\n",
    "    \"\"\"\n",
    "    # placeholder\n",
    "    rois = np.zeros((count, 4), dtype=np.int32)\n",
    "\n",
    "    # Generate random ROIs around GT boxes (90% of count)\n",
    "    rois_per_box = int(0.9 * count / gt_boxes.shape[0])\n",
    "    for i in range(gt_boxes.shape[0]):\n",
    "        gt_y1, gt_x1, gt_y2, gt_x2 = gt_boxes[i]\n",
    "        h = gt_y2 - gt_y1\n",
    "        w = gt_x2 - gt_x1\n",
    "        # random boundaries\n",
    "        r_y1 = max(gt_y1 - h, 0)\n",
    "        r_y2 = min(gt_y2 + h, image_shape[0])\n",
    "        r_x1 = max(gt_x1 - w, 0)\n",
    "        r_x2 = min(gt_x2 + w, image_shape[1])\n",
    "\n",
    "        # To avoid generating boxes with zero area, we generate double what\n",
    "        # we need and filter out the extra. If we get fewer valid boxes\n",
    "        # than we need, we loop and try again.\n",
    "        while True:\n",
    "            y1y2 = np.random.randint(r_y1, r_y2, (rois_per_box * 2, 2))\n",
    "            x1x2 = np.random.randint(r_x1, r_x2, (rois_per_box * 2, 2))\n",
    "            # Filter out zero area boxes\n",
    "            threshold = 1\n",
    "            y1y2 = y1y2[np.abs(y1y2[:, 0] - y1y2[:, 1]) >=\n",
    "                        threshold][:rois_per_box]\n",
    "            x1x2 = x1x2[np.abs(x1x2[:, 0] - x1x2[:, 1]) >=\n",
    "                        threshold][:rois_per_box]\n",
    "            if y1y2.shape[0] == rois_per_box and x1x2.shape[0] == rois_per_box:\n",
    "                break\n",
    "\n",
    "        # Sort on axis 1 to ensure x1 <= x2 and y1 <= y2 and then reshape\n",
    "        # into x1, y1, x2, y2 order\n",
    "        x1, x2 = np.split(np.sort(x1x2, axis=1), 2, axis=1)\n",
    "        y1, y2 = np.split(np.sort(y1y2, axis=1), 2, axis=1)\n",
    "        box_rois = np.hstack([y1, x1, y2, x2])\n",
    "        rois[rois_per_box * i:rois_per_box * (i + 1)] = box_rois\n",
    "\n",
    "    # Generate random ROIs anywhere in the image (10% of count)\n",
    "    remaining_count = count - (rois_per_box * gt_boxes.shape[0])\n",
    "    # To avoid generating boxes with zero area, we generate double what\n",
    "    # we need and filter out the extra. If we get fewer valid boxes\n",
    "    # than we need, we loop and try again.\n",
    "    while True:\n",
    "        y1y2 = np.random.randint(0, image_shape[0], (remaining_count * 2, 2))\n",
    "        x1x2 = np.random.randint(0, image_shape[1], (remaining_count * 2, 2))\n",
    "        # Filter out zero area boxes\n",
    "        threshold = 1\n",
    "        y1y2 = y1y2[np.abs(y1y2[:, 0] - y1y2[:, 1]) >=\n",
    "                    threshold][:remaining_count]\n",
    "        x1x2 = x1x2[np.abs(x1x2[:, 0] - x1x2[:, 1]) >=\n",
    "                    threshold][:remaining_count]\n",
    "        if y1y2.shape[0] == remaining_count and x1x2.shape[0] == remaining_count:\n",
    "            break\n",
    "\n",
    "    # Sort on axis 1 to ensure x1 <= x2 and y1 <= y2 and then reshape\n",
    "    # into x1, y1, x2, y2 order\n",
    "    x1, x2 = np.split(np.sort(x1x2, axis=1), 2, axis=1)\n",
    "    y1, y2 = np.split(np.sort(y1y2, axis=1), 2, axis=1)\n",
    "    global_rois = np.hstack([y1, x1, y2, x2])\n",
    "    rois[-remaining_count:] = global_rois\n",
    "    return rois\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fad48c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(KU.Sequence):\n",
    "    \"\"\"An iterable that returns images and corresponding target class ids,\n",
    "        bounding box deltas, and masks. It inherits from keras.utils.Sequence to avoid data redundancy\n",
    "        when multiprocessing=True.\n",
    "\n",
    "        dataset: The Dataset object to pick data from\n",
    "        config: The model config object\n",
    "        shuffle: If True, shuffles the samples before every epoch\n",
    "        augmentation: Optional. An imgaug (https://github.com/aleju/imgaug) augmentation.\n",
    "            For example, passing imgaug.augmenters.Fliplr(0.5) flips images\n",
    "            right/left 50% of the time.\n",
    "        random_rois: If > 0 then generate proposals to be used to train the\n",
    "                     network classifier and mask heads. Useful if training\n",
    "                     the Mask RCNN part without the RPN.\n",
    "        detection_targets: If True, generate detection targets (class IDs, bbox\n",
    "            deltas, and masks). Typically for debugging or visualizations because\n",
    "            in trainig detection targets are generated by DetectionTargetLayer.\n",
    "\n",
    "        Returns a Python iterable. Upon calling __getitem__() on it, the\n",
    "        iterable returns two lists, inputs and outputs. The contents\n",
    "        of the lists differ depending on the received arguments:\n",
    "        inputs list:\n",
    "        - images: [batch, H, W, C]\n",
    "        - image_meta: [batch, (meta data)] Image details. See compose_image_meta()\n",
    "        - rpn_match: [batch, N] Integer (1=positive anchor, -1=negative, 0=neutral)\n",
    "        - rpn_bbox: [batch, N, (dy, dx, log(dh), log(dw))] Anchor bbox deltas.\n",
    "        - gt_class_ids: [batch, MAX_GT_INSTANCES] Integer class IDs\n",
    "        - gt_boxes: [batch, MAX_GT_INSTANCES, (y1, x1, y2, x2)]\n",
    "        - gt_masks: [batch, height, width, MAX_GT_INSTANCES]. The height and width\n",
    "                    are those of the image unless use_mini_mask is True, in which\n",
    "                    case they are defined in MINI_MASK_SHAPE.\n",
    "\n",
    "        outputs list: Usually empty in regular training. But if detection_targets\n",
    "            is True then the outputs list contains target class_ids, bbox deltas,\n",
    "            and masks.\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, config, shuffle=True, augmentation=None,\n",
    "                 random_rois=0, detection_targets=False):\n",
    "\n",
    "        self.image_ids = np.copy(dataset.image_ids)\n",
    "        self.dataset = dataset\n",
    "        self.config = config\n",
    "\n",
    "        # Anchors\n",
    "        # [anchor_count, (y1, x1, y2, x2)]\n",
    "        self.backbone_shapes = compute_backbone_shapes(config, config.IMAGE_SHAPE)\n",
    "        self.anchors = utils.generate_pyramid_anchors(config.RPN_ANCHOR_SCALES,\n",
    "                                                      config.RPN_ANCHOR_RATIOS,\n",
    "                                                      self.backbone_shapes,\n",
    "                                                      config.BACKBONE_STRIDES,\n",
    "                                                      config.RPN_ANCHOR_STRIDE)\n",
    "\n",
    "        self.shuffle = shuffle\n",
    "        self.augmentation = augmentation\n",
    "        self.random_rois = random_rois\n",
    "        self.batch_size = self.config.BATCH_SIZE\n",
    "        self.detection_targets = detection_targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.image_ids) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        b = 0\n",
    "        image_index = -1\n",
    "        while b < self.batch_size:\n",
    "            # Increment index to pick next image. Shuffle if at the start of an epoch.\n",
    "            image_index = (image_index + 1) % len(self.image_ids)\n",
    "\n",
    "            if self.shuffle and image_index == 0:\n",
    "                np.random.shuffle(self.image_ids)\n",
    "\n",
    "            # Get GT bounding boxes and masks for image.\n",
    "            image_id = self.image_ids[image_index]\n",
    "            image, image_meta, gt_class_ids, gt_boxes, gt_masks = \\\n",
    "                load_image_gt(self.dataset, self.config, image_id,\n",
    "                              augmentation=self.augmentation)\n",
    "\n",
    "            # Skip images that have no instances. This can happen in cases\n",
    "            # where we train on a subset of classes and the image doesn't\n",
    "            # have any of the classes we care about.\n",
    "            if not np.any(gt_class_ids > 0):\n",
    "                continue\n",
    "\n",
    "            # RPN Targets\n",
    "            rpn_match, rpn_bbox = build_rpn_targets(image.shape, self.anchors,\n",
    "                                                    gt_class_ids, gt_boxes, self.config)\n",
    "\n",
    "            # Mask R-CNN Targets\n",
    "            if self.random_rois:\n",
    "                rpn_rois = generate_random_rois(\n",
    "                    image.shape, self.random_rois, gt_class_ids, gt_boxes)\n",
    "                if self.detection_targets:\n",
    "                    rois, mrcnn_class_ids, mrcnn_bbox, mrcnn_mask = \\\n",
    "                        build_detection_targets(\n",
    "                            rpn_rois, gt_class_ids, gt_boxes, gt_masks, self.config)\n",
    "\n",
    "            # Init batch arrays\n",
    "            if b == 0:\n",
    "                batch_image_meta = np.zeros(\n",
    "                    (self.batch_size,) + image_meta.shape, dtype=image_meta.dtype)\n",
    "                batch_rpn_match = np.zeros(\n",
    "                    [self.batch_size, self.anchors.shape[0], 1], dtype=rpn_match.dtype)\n",
    "                batch_rpn_bbox = np.zeros(\n",
    "                    [self.batch_size, self.config.RPN_TRAIN_ANCHORS_PER_IMAGE, 4], dtype=rpn_bbox.dtype)\n",
    "                batch_images = np.zeros(\n",
    "                    (self.batch_size,) + image.shape, dtype=np.float32)\n",
    "                batch_gt_class_ids = np.zeros(\n",
    "                    (self.batch_size, self.config.MAX_GT_INSTANCES), dtype=np.int32)\n",
    "                batch_gt_boxes = np.zeros(\n",
    "                    (self.batch_size, self.config.MAX_GT_INSTANCES, 4), dtype=np.int32)\n",
    "                batch_gt_masks = np.zeros(\n",
    "                    (self.batch_size, gt_masks.shape[0], gt_masks.shape[1],\n",
    "                     self.config.MAX_GT_INSTANCES), dtype=gt_masks.dtype)\n",
    "                if self.random_rois:\n",
    "                    batch_rpn_rois = np.zeros(\n",
    "                        (self.batch_size, rpn_rois.shape[0], 4), dtype=rpn_rois.dtype)\n",
    "                    if self.detection_targets:\n",
    "                        batch_rois = np.zeros(\n",
    "                            (self.batch_size,) + rois.shape, dtype=rois.dtype)\n",
    "                        batch_mrcnn_class_ids = np.zeros(\n",
    "                            (self.batch_size,) + mrcnn_class_ids.shape, dtype=mrcnn_class_ids.dtype)\n",
    "                        batch_mrcnn_bbox = np.zeros(\n",
    "                            (self.batch_size,) + mrcnn_bbox.shape, dtype=mrcnn_bbox.dtype)\n",
    "                        batch_mrcnn_mask = np.zeros(\n",
    "                            (self.batch_size,) + mrcnn_mask.shape, dtype=mrcnn_mask.dtype)\n",
    "\n",
    "            # If more instances than fits in the array, sub-sample from them.\n",
    "            if gt_boxes.shape[0] > self.config.MAX_GT_INSTANCES:\n",
    "                ids = np.random.choice(\n",
    "                    np.arange(gt_boxes.shape[0]), self.config.MAX_GT_INSTANCES, replace=False)\n",
    "                gt_class_ids = gt_class_ids[ids]\n",
    "                gt_boxes = gt_boxes[ids]\n",
    "                gt_masks = gt_masks[:, :, ids]\n",
    "\n",
    "            # Add to batch\n",
    "            batch_image_meta[b] = image_meta\n",
    "            batch_rpn_match[b] = rpn_match[:, np.newaxis]\n",
    "            batch_rpn_bbox[b] = rpn_bbox\n",
    "            batch_images[b] = mold_image(image.astype(np.float32), self.config)\n",
    "            batch_gt_class_ids[b, :gt_class_ids.shape[0]] = gt_class_ids\n",
    "            batch_gt_boxes[b, :gt_boxes.shape[0]] = gt_boxes\n",
    "            batch_gt_masks[b, :, :, :gt_masks.shape[-1]] = gt_masks\n",
    "            if self.random_rois:\n",
    "                batch_rpn_rois[b] = rpn_rois\n",
    "                if self.detection_targets:\n",
    "                    batch_rois[b] = rois\n",
    "                    batch_mrcnn_class_ids[b] = mrcnn_class_ids\n",
    "                    batch_mrcnn_bbox[b] = mrcnn_bbox\n",
    "                    batch_mrcnn_mask[b] = mrcnn_mask\n",
    "            b += 1\n",
    "\n",
    "        inputs = [batch_images, batch_image_meta, batch_rpn_match, batch_rpn_bbox,\n",
    "                  batch_gt_class_ids, batch_gt_boxes, batch_gt_masks]\n",
    "        outputs = []\n",
    "\n",
    "        if self.random_rois:\n",
    "            inputs.extend([batch_rpn_rois])\n",
    "            if self.detection_targets:\n",
    "                inputs.extend([batch_rois])\n",
    "                # Keras requires that output and targets have the same number of dimensions\n",
    "                batch_mrcnn_class_ids = np.expand_dims(\n",
    "                    batch_mrcnn_class_ids, -1)\n",
    "                outputs.extend(\n",
    "                    [batch_mrcnn_class_ids, batch_mrcnn_bbox, batch_mrcnn_mask])\n",
    "\n",
    "        return inputs, outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
